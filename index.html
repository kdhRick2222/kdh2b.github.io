<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Your Name</title>
    <style>
       body {
            font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
            text-align: center;
            margin: 0;
            padding: 50px;
            background-color: #f9f9f9;
            display: flex;
            flex-direction: column;
            align-items: center;
            line-height: 2;
        }
        .profile {
            display: flex;
            align-items: center;
            max-width: 800px;
            text-align: left;
        }
        .profile img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            margin-right: 20px;
        }
        .profile-text {
            max-width: 600px;
        }
        h1 {
            font-size: 2.5em;
        }
        p {
            font-size: 1.2em;
            color: #555;
        }
        .links a {
            display: inline-block;
            margin: 15px;
            text-decoration: none;
            color: #0073e6;
            font-size: 1.3em;
        }
        .publications {
            margin-top: 60px;
            max-width: 1000px;
            text-align: left;
            width: 90%;
        }
        .publications h2 {
            font-size: 2.2em;
            text-align: center;
        }
        .publication-item {
            display: flex;
            flex-direction: row;
            align-items: flex-start;
            margin-bottom: 30px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 25px;
        }
        .publication-image {
            width: 280px;
            height: 180px;
            margin-right: 25px;
            object-fit: cover;
        }
        .publication-description {
            flex: 1;
            font-size: 1.1em;
            color: #333;
            padding-right: 20px;
        }
        .publication-details {
            flex: 2;
            font-size: 1.1em;
        }
        .publication-details a {
            font-size: 1.3em;
            text-decoration: none;
            color: #0073e6;
        }
    </style>
</head>
<body>
    <div class="profile">
        <img src="images/my_picture.png" alt="Your Photo">
        <div class="profile-text">
            <h1>Hello, I'm Daehyun Kim</h1>
            <p>AI & Computer Vision</p>
            <p>I am a PhD student in Artificial Intelligence department at the Hanyang University in Seoul, researching low-light image enhancement, exposure correction, and anomaly detection. I am advised by Prof. Taehyun Kim</p>
        </div>
    </div>
    <div class="links">
        <a href="https://github.com/kdhRick2222">GitHub</a>
        <a href="https://scholar.google.com/citations?user=gctcrlcAAAAJ&hl=ko">Google Scholar</a>
        <a href="daehyun@hanyang.ac.kr">Email</a>
    </div>
    <div class="publications">
        <h2>Publications</h2>
        <div class="publication-item">
            <img src="images/clode_2025_.png" alt="Paper 5 Image" class="publication-image">
            <div class="publication-details">
                <a href="https://openreview.net/pdf?id=5p57uCUH8k">CLODE: Continuous Exposure Learning for Low-Light Image Enhancement using Neural ODEs</a> <br><em><b>ICLR, 2025 (spotlight)</b></em> <br><b>Daehyun Kim*</b>, Donggoo Jung*, Tae Hyun Kim <br> In this work, we focus on the strength of curve-adjustment-based approaches to tackle unsupervised methods. The majority of existing unsupervised curve-adjustment approaches iteratively estimate higher order curve parameters to enhance the exposure of images while efficiently preserving the details of the images. However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance. To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image. 
            </div>
        </div>
        <div class="publication-item">
            <img src="images/sanflow_2023.png" alt="Paper 4 Image" class="publication-image">
            <div class="publication-details">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ee74a6ade401e200985e2421b20bbae4-Abstract-Conference.html">SANFlow: Semantic-Aware Normalizing Flow for Anomaly Detection</a> <br><em><b>NeurIPS, 2023</b></em> <br><b>Daehyun Kim</b>, Sungyong Baik, Tae Hyun Kim <br> Previous NF-based methods have relied solely on the capability of NF and forcibly transformed the distribution of all features to a single distribution (eg, unit normal distribution), when features can have different semantic information and thus follow different distributions. We claim that forcibly learning to transform such diverse distributions to a single distribution with a single network will cause the learning difficulty, limiting the capacity of a network to discriminate normal and abnormal data. As such, we propose to transform the distribution of features at each location of a given image to different distributions. In particular, we train NF to map normal data distribution to distributions with the same mean but different variances at each location of the given image. To enhance the discriminability, we also train NF to map abnormal data distribution to a distribution with a mean that is different from that of normal data, where abnormal data is synthesized with data augmentation. 
            </div>
        </div>
        <div class="publication-item">
            <div class="publication-details">
                <a href="https://link.springer.com/article/10.1007/s00431-024-05505-7">Machine learning-based analysis for prediction of surgical necrotizing enterocolitis in very low birth weight infants using perinatal factors: a nationwide cohort study</a> <br><em>European Journal of Pediatrics, 2024</em> <br>Seung Hyun Kim, Yoon Ju Oh, Joonhyuk Son, Donggoo Jung, <b>Daehyun Kim</b>, Soo Rack Ryu, Jae Yoon Na, Jae Kyoon Hwang, Tae Hyun Kim, Hyun-Kyung Park
            </div>
        </div>
        <div class="publication-item">
<!--             <img src="images/Neonatology_2023.png" alt="Paper 3 Image" class="publication-image"> -->
            <div class="publication-details">
                <a href="https://karger.com/neo/article/120/5/652/854411">Learning-based longitudinal prediction models for mortality risk in very-low-birth-weight infants: a nationwide cohort study</a> <br><em>Neonatology, 2023</em> <br>Jae Yoon Na*, Donggoo Jung*, Jong Ho Cha, <b>Daehyun Kim</b>, Joonhyuk Son, Jae Kyoon Hwang, Tae Hyun Kim, Hyun-Kyung Park
            </div>
        </div>
        <div class="publication-item">
<!--             <img src="images/frontiers_2023.png" alt="Paper 2 Image" class="publication-image"> -->
            <div class="publication-details">
                <a href="https://www.frontiersin.org/journals/pediatrics/articles/10.3389/fped.2023.1155921/full">Two-stage learning-based prediction of bronchopulmonary dysplasia in very low birth weight infants: a nationwide cohort study</a> <br><em>Frontiers in Pediatrics, 2023</em> <br>Jae Kyoon Hwang*, <b>Dae Hyun Kim*</b>, Jae Yoon Na*, Joonhyuk Son, Yoon Ju Oh, Donggoo Jung, Chang-Ryul Kim, Tae Hyun Kim, Hyun-Kyung Park
            </div>
        </div>
        <div class="publication-item">
<!--             <img src="images/scientificreports_2022.png" alt="Paper 1 Image" class="publication-image"> -->
            <div class="publication-details">
                <a href="https://www.nature.com/articles/s41598-022-16273-5">Development of artificial neural networks for early prediction of intestinal perforation in preterm infants.</a> <br><em>Scientific Reports, 2022</em> <br>Joonhyuk Son*, <b>Daehyun Kim*</b>, Jae Yoon Na*, Donggoo Jung, Ja-Hye Ahn, Tae Hyun Kim, Hyun-Kyung Park
            </div>
        </div>
        * indicates equal contribution
    </div>
</body>
</html>

